# Inference server settings
inference:
  host: "0.0.0.0"
  port: 8080
  workers: 2
  max_batch_size: 32
  timeout_seconds: 30
  prometheus_metrics_port: 8000

model:
  artifact_path: "/models"
  model_file: "ensemble.joblib"
  scaler_file: "scaler.joblib"
  reload_interval_seconds: 300

logging:
  level: "INFO"
  format: "json"